import json
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# === Configuration gÃ©nÃ©rale ===
MODEL_NAME = "nomic-ai/nomic-embed-text-v1"
OUTPUT_DIR = Path("output")
EMBEDDING_DIM = 768

# === Chargement du modÃ¨le ===
print("ğŸ”„ Chargement du modÃ¨le d'embedding...")
model = SentenceTransformer(MODEL_NAME)
print(f"âœ… ModÃ¨le chargÃ© : {MODEL_NAME}")

# === Initialisation de FAISS ===
index = faiss.IndexFlatL2(EMBEDDING_DIM)
metadata = []

# === Traitement de chaque document ===
doc_folders = list(OUTPUT_DIR.iterdir())
print(f"\nğŸ“ {len(doc_folders)} documents Ã  traiter...\n")

for doc_folder in tqdm(doc_folders, desc="ğŸ“š Embedding des documents"):
    chunk_path = doc_folder / "cleaned_chunks.jsonl"
    
    if not chunk_path.exists():
        print(f"âš ï¸  IgnorÃ© : {doc_folder.name} â†’ Aucun 'cleaned_chunks.jsonl' trouvÃ©.")
        continue

    print(f"\nğŸ“„ Traitement : {doc_folder.name}")
    
    with open(chunk_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            chunk_text = data["text"]
            chunk_id = data["chunk_id"]

            embedding = model.encode(chunk_text)
            embedding_np = np.array([embedding])

            index.add(embedding_np)
            metadata.append({
                "doc_name": doc_folder.name,
                "chunk_id": chunk_id,
                "text": chunk_text[:100]
            })

# === Sauvegardes ===
print(f"\nâœ… Embedding terminÃ©. Total de vecteurs embarquÃ©s : {index.ntotal}")

faiss.write_index(index, "faiss_index.bin")
print("ğŸ’¾ Index FAISS sauvegardÃ© â†’ faiss_index.bin")

with open("faiss_metadata.json", "w", encoding="utf-8") as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)
print("ğŸ’¾ MÃ©tadonnÃ©es sauvegardÃ©es â†’ faiss_metadata.json")

print("\nğŸ‰ Tous les documents ont Ã©tÃ© indexÃ©s avec succÃ¨s !")
